# -*- coding: utf-8 -*-
"""sm_audit

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10lcF-cPpQxjtMXvjJNiKeseBCczAoAPu

# Importing Libraries
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import os
import re
import dateutil.parser
from google.colab import drive, auth
from gspread_dataframe import set_with_dataframe
from google.auth import default
from datetime import datetime
from dateutil.relativedelta import *
from google.colab import data_table
data_table.enable_dataframe_formatter()

!pip install fuzzywuzzy
!pip install python-Levenshtein
from fuzzywuzzy import fuzz, process

# Mount Google Drive
drive.mount('/content/drive')

# Authenticate user for Google Sheets
auth.authenticate_user()

# Authorize Google Sheets API
import gspread
from google.auth import default
creds, _ = default()
gc = gspread.authorize(creds)

# Define utility functions
def changeDateFormat(row):
    return pd.to_datetime(row, errors='coerce').dt.date

def cleanText(row):
    return row.str.upper().str.strip()

# Define function to parse date and time formats
def parseDateTimeFormatTZ(row):
    return dateutil.parser.parse(row)

# Define function to filter data
def filtered(x):
  return [i for i in x if i!='Not Sold']

def filtered1(x):
  return [i for i in x if i=='YTB']

# Define function to get and merge dump files within a date range
def getDump(mastermerge, dump_name, start_date, end_date):
    start_date = pd.to_datetime(start_date).date()
    end_date = pd.to_datetime(end_date).date()
    file_path = "/content/drive/MyDrive/Insights_Datalake/Datewise Dumps - Aug Onwards/"
    reg = re.compile(".+" + dump_name + ".csv")

    for (root, dirs, files) in os.walk(file_path):
        for file in files:
            path = os.path.join(root, file)
            date = pd.to_datetime(path.split('/')[7:][0]).date()

            if bool(re.match(reg, path)) and (start_date <= date <= end_date):
                temp = pd.read_csv(path, parse_dates=True)
                temp['DumpDate'] = date
                read_dump_name = path.split('/')[9:][0]
                print(f"{read_dump_name} on Date: {date} Dump of shape: {temp.shape} read from path: {path}")
                mastermerge = mastermerge.append(temp)

    mastermerge[['DumpDate']] = mastermerge[['DumpDate']].apply(changeDateFormat)
    return mastermerge

#get data from spreadsheetID and Range Name function
def get_data(spreadSheetId, rangeName):
    wb = gc.open_by_key(spreadSheetId)
    ws = wb.worksheet(rangeName)
    rows = ws.get_all_values()
    return rows
#get all data in a sheet as df and there 1st row things as the df labels.
#If sheets don't have their labels in the first row than we have to do it manuaaly as per the data
def get_data_df(spreadSheetId, rangeName):
  l = get_data(spreadSheetId, rangeName)
  ldf = pd.DataFrame.from_records(l[1:], columns = l[0])
  return ldf

"""# Applying Conditions"""

#reading the data set
spreadSheetId="1aVAu23rPNHyOEk2ioNXdbqyUKEFzcXedlY283cvDKvY"
rangeName="Society Master"
sm=get_data_df(spreadSheetId, rangeName)

def convert_to_datetime(date_str):
    try:
        # Try the format '%d-%b-%Y'
        return pd.to_datetime(date_str, format='%d-%b-%Y')
    except ValueError:
        try:
            # Try a different format if the first one fails
            return pd.to_datetime(date_str, format='%Y-%m-%d')
        except ValueError:
            return None  # Return None for invalid dates


# Apply the custom function to the 'date_column'
sm['Closure Date (VMS)'] = sm['Closure Date (VMS)'].apply(convert_to_datetime)
sm['Closure Date (ERP)'] = sm['Closure Date (ERP)'].apply(convert_to_datetime)
sm['Latest Signing Date']=sm['Latest Signing Date'].apply(convert_to_datetime)

#Conditons
#sm[((sm['Closure Type (VMS)']=='New Closure')&(sm['Original Closure Number']!=sm['Closure Number (VMS)'])) | ((sm['Closure Type (ERP)']=='New Closure')&(sm['Original Closure Number']!=sm['Closure Number (ERP)']))]
sm=sm[((sm['Closure Type (VMS)']=='New Closure')&(sm['Original Closure Number']!=sm['Closure Number (VMS)'])) | ((sm['Closure Type (ERP)']=='New Closure')&(sm['Original Closure Number']!=sm['Closure Number (ERP)']))]
sm=sm[sm['Latest Signing Date']> pd.to_datetime('2021-04-30')]
sm=sm[['City', 'Kibana ID', 'Original Closure Number', 'Society Name','societyStatus', 'Live Product', 'Latest Signing Date','Sold Flats (VMS)', 'Sold Flats (ERP)', 'Sold Flats','Closure Number (VMS)', 'Closure Type (VMS)', 'Closure Date (VMS)','VMSHODate', 'HO Product (VMS)', 'VMS Onb Status', 'VMSOnbDate','VMSChurnDate', 'Closure Number (ERP)', 'Closure Date (ERP)','Closure Type (ERP)', 'ERPHODate', 'HO Product (ERP)', 'ERP Onb Status','ERPOnbDate', 'ERPChurnDate','Open to Sales Status-ERP','Open to Sales Status-VMS']]

#sm=sm[((sm['Closure Number (VMS)'].duplicated(keep=False))&(sm['Closure Number (VMS)']!=''))|((sm['Closure Number (ERP)'].duplicated(keep=False))&(sm['Closure Number (ERP)']!=''))]

#reading the static data set
spreadSheetId="1yt8MtGP-_5SlZhRkS6dyALmDY-eV99IhgBT-6MSaQno"
rangeName="Sheet1"
sm_static=get_data_df(spreadSheetId, rangeName)

#checking the new data
sm=sm[~sm['Kibana ID'].isin(sm_static['Kibana ID'])]

#Appending the new Values
sm_final=pd.concat([sm_static, sm], ignore_index=True)

"""# Pushing the Data"""

#Pushing Data to the sheet
spreadSheetId="1yt8MtGP-_5SlZhRkS6dyALmDY-eV99IhgBT-6MSaQno"
rangeName="Sheet1"

wb = gc.open_by_key(spreadSheetId)
ws = wb.worksheet(rangeName)

column = 1
row = 1
wb.values_clear("Sheet2")
df = pd.DataFrame(sm_final)
set_with_dataframe(ws, df, row=row, col=column, include_index=False, include_column_header=True)

"""# Anuja Checker"""

from datetime import datetime
import pytz


# Get the current time in UTC
utc_time = datetime.utcnow()


# Convert to India Standard Time (IST)
india_tz = pytz.timezone('Asia/Kolkata')
ist_time = utc_time.replace(tzinfo=pytz.utc).astimezone(india_tz)


# Format the datetime string
formatted_time = ist_time.strftime("%d-%B-%Y - %H:%M")


print(formatted_time)


import gspread
import pandas as pd
from datetime import datetime
from gspread_dataframe import set_with_dataframe


# Open the Google Sheets document
#gc = gspread.service_account(filename="path/to/your/credentials.json")
spreadsheet_id = "12jy3u_vMDIS0etAQe1DYa14mBUwXebDc9ePzgzgkTH0"
date_checker = gc.open_by_key(spreadsheet_id)


# Function to update target value
def update_target_value(target_value):
   # Find the worksheet 'scripts' and fetch data
   worksheet_scripts = date_checker.worksheet('last updated time')
   data_scripts = worksheet_scripts.get_all_records()


   # Create a DataFrame with only the 'Name ' column
   df = pd.DataFrame(data_scripts, columns=['Name '])


   # Find the row index for the target value
   filtered_row = df[df['Name '] == target_value]


   if not filtered_row.empty:
       # Get the index of the found row
       row_index = filtered_row.index[0]


       # Get the 'last updated time' worksheet
       worksheet_last_updated = date_checker.worksheet('last updated time')


       # Get the current datetime
       current_datetime = formatted_time


       # Update the 'last updated date / time' column with the current datetime
       worksheet_last_updated.update(f'B{row_index + 2}', [[current_datetime]])


       print(f"Current datetime {current_datetime} populated for Value {target_value}")
   else:
       print(f"No matching row found for Value {target_value}")


update_target_value('Societies with Uniview number not matching ')